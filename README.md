# Transformer-Encoder
![Transformer Encoder](https://user-images.githubusercontent.com/7080342/59344525-8e6bd180-8d0e-11e9-8fa6-0ae7ae15d1e2.png)
## Intro
The transformer architecture was first proposed in the paper [Attention is all you need](https://arxiv.org/abs/1706.03762) by google researcher. It uses attention to self attend the input to increase the accuracy. The biggest point of this architecture is that it designed to have an high degree of parallelization so it is indicated to be run in large training operations. The transformer encoder was used in important researches like [Bert](https://arxiv.org/abs/1810.04805) and [OpenGPT1](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) and [OpenGPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
