{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention:\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        self.scale_factor = tf.sqrt(float(dim))\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        K = tf.transpose(K, perm=[0,2,1])\n",
    "        softmax_scaled_weights = tf.nn.softmax(tf.matmul(Q,K) / self.scale_factor)\n",
    "        return tf.matmul(softmax_scaled_weights, V)\n",
    "    \n",
    "class MultiHeadAttention:\n",
    "    \n",
    "    def __init__(self, heads, hidden_inp, trainable=True):\n",
    "        assert(hidden_inp % heads == 0)\n",
    "        \n",
    "        self.trainable = trainable\n",
    "        self.heads = heads\n",
    "        self.h_in = hidden_inp\n",
    "        self.dk = hidden_inp // heads\n",
    "        self.t_shape = (self.h_in, self.dk * self.heads)\n",
    "        self.scaled_dpa = ScaledDotProductAttention(self.dk)\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.Wq = tf.Variable(tf.random_normal(self.t_shape))\n",
    "        self.Wk = tf.Variable(tf.random_normal(self.t_shape))\n",
    "        self.Wv = tf.Variable(tf.random_normal(self.t_shape))\n",
    "        self.Wo = tf.Variable(tf.random_normal((self.h_in, self.dk * self.heads)))\n",
    "        self._trainable_weights = [self.Wq, self.Wk, self.Wv, self.Wo]\n",
    "        \n",
    "    @tf.contrib.eager.defun\n",
    "    def forward(self, Q, K, V):\n",
    "        \n",
    "        # input dims [batch, ts, dk*heads]\n",
    "        q = tf.tensordot(Q, self.Wq, axes=[[-1], [0]])\n",
    "        k = tf.tensordot(K, self.Wk, axes=[[-1], [0]])\n",
    "        v = tf.tensordot(V, self.Wv, axes=[[-1], [0]])\n",
    "        \n",
    "        def reshape1(x):\n",
    "            s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "            x = tf.reshape(x, [s[0], s[1], self.heads, self.dk])\n",
    "            x = tf.transpose(x, [2, 0, 1, 3])  \n",
    "            x = tf.reshape(x, [-1, s[1], self.dk])  # [n_head * batch_size, len_q, dk]\n",
    "            return x\n",
    "        \n",
    "        # Reshape to do the scaled dot product attention to [batch*heads, ts, dk]\n",
    "        q = reshape1(q)\n",
    "        k = reshape1(k)\n",
    "        v = reshape1(v)\n",
    "        \n",
    "        dp_att_out = self.scaled_dpa.forward(q,k,v)\n",
    "        \n",
    "        # Reshape back to [batch, ts, dk*heads]\n",
    "        def reshape2(x):\n",
    "            s = tf.shape(x)   # [n_head * batch_size, len_v, d_k]\n",
    "            x = tf.reshape(x, [self.heads, -1, s[1], s[2]]) \n",
    "            x = tf.transpose(x, [1, 2, 0, 3])\n",
    "            x = tf.reshape(x, [-1, s[1], self.heads*self.dk])  # [batch_size, len_v, n_head * d_k]\n",
    "            return x\n",
    "        \n",
    "        dp_att_out = reshape2(dp_att_out)\n",
    "        \n",
    "        return tf.tensordot(dp_att_out, self.Wo, axes=[[-1], [0]])\n",
    "\n",
    "    \n",
    "    def get_trainable_weights(self):\n",
    "        return self._trainable_weights if self.trainable else []\n",
    "\n",
    "class Dense:\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, bias=True, trainable=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.bias = bias\n",
    "        self.trainable = trainable\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.W = tf.Variable(tf.random_normal((self.input_dim, self.output_dim)))\n",
    "        if self.bias:\n",
    "            self.b  = tf.Variable(tf.random_uniform((self.output_dim,)))\n",
    "            self._trainable_weights = [self.W, self.b]\n",
    "        else:\n",
    "            self._trainable_weights = [self.W]\n",
    "   \n",
    "    @tf.contrib.eager.defun\n",
    "    def forward(self, input_):\n",
    "        if self.bias:\n",
    "            return tf.tensordot(input_, self.W, axes=[[-1], [0]]) + self.b\n",
    "        else:\n",
    "            return tf.tensordot(input_, self.W, axes=[[-1], [0]])\n",
    "        \n",
    "    def get_trainable_weights(self):\n",
    "        return self._trainable_weights if self.trainable else []\n",
    "    \n",
    "\n",
    "class TransformerLayer:\n",
    "    \n",
    "    def __init__(self, heads, hidden_inp, dense_hidden, trainable=True):\n",
    "        self.trainable = trainable\n",
    "        self.heads = heads\n",
    "        self.hidden_inp = hidden_inp\n",
    "        self.hidden_output = hidden_inp // heads\n",
    "        self.dense_hidden = dense_hidden\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.mha = MultiHeadAttention(self.heads, self.hidden_inp)\n",
    "        self.dense1 = Dense(self.hidden_inp, self.dense_hidden)\n",
    "        self.dense2 = Dense(self.dense_hidden, self.hidden_inp)\n",
    "        self._trainable_variables = self.mha.get_trainable_weights() + [self.dense1, self.dense2]\n",
    "        \n",
    "    @tf.contrib.eager.defun\n",
    "    def forward(self, Q, K, V):\n",
    "        output_ma = self.mha.forward(Q, K, V)\n",
    "        o_ma_norm = tf.contrib.layers.layer_norm(output_ma + K)\n",
    "        o_dense1 = tf.nn.relu(self.dense1.forward(o_ma_norm))\n",
    "        o_dense2 = self.dense2.forward(o_dense1)\n",
    "        output = tf.contrib.layers.layer_norm(o_dense2 + o_ma_norm)\n",
    "        return output\n",
    "        \n",
    "    def get_trainable_weights(self):\n",
    "        return self._trainable_variables if self.trainable else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder:\n",
    "    \n",
    "    def __init__(self, num_layers, heads, embedding_dim, fc_hidden_dim, num_classes):\n",
    "        self.num_layers = num_layers\n",
    "        self.heads = heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dense_hidden = fc_hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.layers = []\n",
    "        for _ in range(self.num_layers):\n",
    "            self.layers += [TransformerLayer(self.heads, self.embedding_dim, self.dense_hidden)]\n",
    "        \n",
    "        self.dense_out = Dense(self.embedding_dim, self.num_classes, bias=False)\n",
    "        self.layers += [self.dense_out]\n",
    "    \n",
    "    @tf.contrib.eager.defun\n",
    "    def forward(self, input_):\n",
    "        o_step_i = input_\n",
    "        for layer in self.layers[:-1]:\n",
    "            o_step_i = layer.forward(o_step_i,o_step_i,o_step_i)\n",
    "\n",
    "        return self.dense_out.forward(o_step_i)\n",
    "    \n",
    "    def get_trainable_weights(self):\n",
    "        return [weight for layer in self.layers for weight in layer.get_trainable_weights()]\n",
    "    \n",
    "    def train(self,\n",
    "              x_train, \n",
    "              y_train, \n",
    "              x_val, \n",
    "              y_val, \n",
    "              vocab,\n",
    "              loss, \n",
    "              epochs,\n",
    "              score_fun,\n",
    "              tensorboard=False,\n",
    "              log_dir=\"./transformer_log/\",\n",
    "              ckpt_dir=\"./transformer_ckpt/\",\n",
    "              pad_value=0,\n",
    "              batch_size=32, \n",
    "              val_bs=32):\n",
    "        \n",
    "        if tensorboard:\n",
    "            summary_writer = tf.contrib.summary.create_file_writer(log_dir, flush_millis=10000)\n",
    "            summary_writer.set_as_default()\n",
    "            global_step = tf.train.get_or_create_global_step()\n",
    "            \n",
    "        iteration = 0\n",
    "        n_classes = y_train.shape[1]\n",
    "        current_val_score = self.compute_score(x_val, y_val, vocab, n_classes, val_bs, score_fun)\n",
    "        for epoch in range(epochs):\n",
    "            for x, y in get_embedded_iterator(x_train, y_train, n_classes, batch_size, vocab):\n",
    "                \n",
    "                if tensorboard:\n",
    "                    global_step.assign_add(1)\n",
    "                \n",
    "                minimize(self.optimizer, self, loss, x, y, lr=0.001, logging=tensorboard, it=iteration, log_every=10)\n",
    "            \n",
    "            val_score = self.compute_score(x_val, y_val, vocab, n_classes, val_bs, score_fun)\n",
    "\n",
    "            if tensorboard:\n",
    "                log_scalar('val_score', val_score)\n",
    "\n",
    "            print(\"Validation score is {0}\".format(val_score))\n",
    "            \n",
    "            if val_score > current_val_score:\n",
    "                self.save_model(ckpt=ckpt_dir)\n",
    "                \n",
    "    def compute_score(self, x_val, y_true, vocab, n_classes, bs, score_fun):\n",
    "        scores = []\n",
    "        \n",
    "        for x, y in get_embedded_iterator(x_val, y_true, n_classes, bs, vocab):\n",
    "            scores.append(score_fun(self.forward(x), y))\n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def save_model(self, ckpt=\"./transformer_log/\"):\n",
    "        self.ckp.save(ckpt)\n",
    "\n",
    "    def restore_model(self, ckpt=\"./transformer_ckpt/\"):\n",
    "        self.ckp.restore(tf.train.latest_checkpoint(ckpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(8, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mha.forward(tf.random_normal((10,12,512)), tf.random_normal((10,12,512)), tf.random_normal((10,12,512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_layer = TransformerLayer(8, 512, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "t_layer.forward(tf.random_normal((10,2,512)), tf.random_normal((10,2,512)), tf.random_normal((10,2,512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_encoder = TransformerEncoder(6, 8, 512, 2048, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.99 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7332, shape=(128, 182, 2), dtype=float32, numpy=\n",
       "array([[[-33.881607  ,  -2.3976965 ],\n",
       "        [-25.505833  ,  -5.985231  ],\n",
       "        [-38.95021   ,  -3.8312526 ],\n",
       "        ...,\n",
       "        [-38.821518  ,   7.856987  ],\n",
       "        [-25.54805   ,  -2.078445  ],\n",
       "        [-22.891998  ,  -1.8945389 ]],\n",
       "\n",
       "       [[ -9.196106  ,   8.707629  ],\n",
       "        [-23.193325  ,   7.910781  ],\n",
       "        [-17.789616  ,  10.899169  ],\n",
       "        ...,\n",
       "        [-28.601572  ,  -2.873456  ],\n",
       "        [-20.607645  ,   1.9075794 ],\n",
       "        [-12.547087  ,  -3.5973682 ]],\n",
       "\n",
       "       [[-21.083778  ,  -0.2351141 ],\n",
       "        [-22.88121   ,   6.1116323 ],\n",
       "        [-29.783657  ,   9.990348  ],\n",
       "        ...,\n",
       "        [-14.516047  ,   1.4761353 ],\n",
       "        [-18.951696  ,  -0.8923621 ],\n",
       "        [-29.320147  ,   0.24620867]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-13.379937  ,  -4.368475  ],\n",
       "        [-13.5032015 ,  -7.1722164 ],\n",
       "        [-16.199566  ,  -6.9870105 ],\n",
       "        ...,\n",
       "        [-16.495617  , -10.109959  ],\n",
       "        [-11.504489  ,  -1.2450838 ],\n",
       "        [-14.373625  ,   0.4677949 ]],\n",
       "\n",
       "       [[-25.15046   ,  -8.317607  ],\n",
       "        [-20.587662  , -11.256458  ],\n",
       "        [-24.754997  , -22.963085  ],\n",
       "        ...,\n",
       "        [-25.794565  ,   1.555996  ],\n",
       "        [-14.734328  ,  -6.928917  ],\n",
       "        [-20.302794  ,  -5.7723494 ]],\n",
       "\n",
       "       [[-13.345861  , -20.40717   ],\n",
       "        [ -4.757682  , -16.992167  ],\n",
       "        [-12.610453  , -24.430586  ],\n",
       "        ...,\n",
       "        [  4.5511355 , -18.10642   ],\n",
       "        [ -7.4720993 , -16.45005   ],\n",
       "        [-10.087881  , -23.906345  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "t_encoder.forward(tf.random_normal((128,182,512)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
